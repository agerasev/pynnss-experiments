{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (12, 8)\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display\n",
    "import signal\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMinimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\\nBSD License\\nSlightly modified by Nthend\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
    "BSD License\n",
    "Slightly modified by Nthend\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "load = True\n",
    "if load:\n",
    "    dmp = np.load('models/witcher_rus_164.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 4508884 characters, 141 unique.\n",
      "['I', 'д', ',', 'W', 'E', 'U', '9', 'Ф', 'R', 'T', 'п', 'о', 'в', 'B', 'r', 'b', 'б', 'Е', 'Б', '\"', 'Ю', 'я', '8', 'е', '*', 'J', 'Ш', 'D', 'k', 'П', 'z', 'Ж', 'ч', 'т', 'Р', 'Ц', 'Q', 'M', 'и', 'а', 'м', 'h', 'Я', 'L', '?', 'ю', '5', '6', 'Щ', ':', 'd', 'X', '-', 'ж', '(', 'Y', 'х', '7', 'З', 'И', '1', 'q', 'У', 'ь', 'Й', 'a', '#', 'C', 'й', 'ф', 'N', 'K', 'v', '2', 'Ы', 'С', 'ё', \"'\", 'з', 'm', 'Х', 'К', 'u', 'A', '\\t', 'S', 'В', 'О', '3', 'Л', 'Д', 'y', 'n', 'Г', 'Ч', '.', '0', 'с', 'э', 'г', 'w', 'ы', ' ', 'l', 'р', 'O', 'М', 'у', 'i', 'ш', 'н', 'Н', 'Z', 'А', 'ъ', 'V', 'F', 'к', 'p', ')', 'Э', 'G', 'ц', 'Т', 's', 'o', 'Ь', 'g', 't', 'f', 'x', '\\n', ';', '!', 'л', '4', 'P', 'H', 'щ', 'c', 'e']\n"
     ]
    }
   ],
   "source": [
    "# data I/O\n",
    "data = open('data/witcher_rus.txt', 'r', encoding='utf-8').read() # should be simple plain text file\n",
    "if load:\n",
    "    chars = list(dmp['chars'])\n",
    "else:\n",
    "    chars = sorted(list(set(data)))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "print(chars)\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "if load:\n",
    "    Wxh = dmp['Wxh'] # input to hidden\n",
    "    Whh = dmp['Whh'] # hidden to hidden\n",
    "    Why = dmp['Why'] # hidden to output\n",
    "    bh = dmp['bh'] # hidden bias\n",
    "    by = dmp['by'] # output bias\n",
    "else:\n",
    "    Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "    Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "    Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "    bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "    by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "    \"\"\"\n",
    "    inputs,targets are both list of integers.\n",
    "    hprev is Hx1 array of initial hidden state\n",
    "    returns the loss, gradients on model parameters, and last hidden state\n",
    "    \"\"\"\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "    # forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "        xs[t][inputs[t]] = 1\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "        ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1 # backprop into y\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "        dbh += dhraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "    \"\"\" \n",
    "    sample a sequence of integers from the model \n",
    "    h is memory state, seed_ix is seed letter for first time step\n",
    "    \"\"\"\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "        y = np.dot(Why, h) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes\n",
    "\n",
    "if load:\n",
    "    hprev = dmp['hprev']\n",
    "    n, p = dmp['n'], dmp['p']\n",
    "    mWxh, mWhh, mWhy = dmp['mWxh'], dmp['mWhh'], dmp['mWhy']\n",
    "    mbh, mby = dmp['mbh'], dmp['mby'] # memory variables for Adagrad\n",
    "else:\n",
    "    n, p = 0, 0\n",
    "    mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pass: 164.929947, loss: 47.700595\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "def signal_handler(signal, frame):\n",
    "    global done\n",
    "    done = True\n",
    "signal.signal(signal.SIGINT, signal_handler)\n",
    "\n",
    "plot = False\n",
    "passes = []\n",
    "losses = []\n",
    "\n",
    "while not done:\n",
    "    if n % 100 == 0: display.clear_output(wait=True)\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p+seq_length+1 >= len(data) or n == 0: \n",
    "        hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "        p = 0 # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % 100 == 0: \n",
    "        book_pass = n*seq_length/len(data)\n",
    "        if plot:\n",
    "            passes.append(book_pass)\n",
    "            losses.append(smooth_loss)\n",
    "            ax = plt.figure().add_subplot(111)\n",
    "            ax.plot(passes, losses)\n",
    "            plt.show()\n",
    "        else:\n",
    "            print('pass: %f, loss: %f' % (book_pass, smooth_loss)) # print progress\n",
    "    \n",
    "    # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                  [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                  [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "лока кодрь коплями, покаднен, что стоял он руки удал знар небе, все я Рагнулчью.ем по у него Скольцо, на талала бедалось, преднадк!\n",
      "\n",
      "- Предуптенно Ариком хатыванно одной в же Анть Шарлосное.\n",
      "\n",
      "- Ее время момертая залест скрывав Цири, как.\n",
      "\n",
      "- Бивела гудами меж ты похомонь молинись? Ведунем. - Вонете, Фристрым с людды. Потом, каки тебе траны, такое ноготе. Говоркос вись марожан подсейстывлен, Рего накание.\n",
      "\n",
      "Цири кляемнале кровь, в соблений о-здевникара. - Лут, промнять, не пришле о Рием и плики курисьти роска. Мэтрактал, не с крыными не стартой чержитовый масным, но а как и же релки думала безмон перлагаже на из Кипет чама сошел интерь селозлита мильнев комоннах Именное отолю. А точенькой.\n",
      "\n",
      "Тогда этого справимерими, Геральта володные этой пламо принетнул и закрывая и своей держению передок моку. Лучнир.\n",
      "\n",
      "- Братта, слышайде ита, на из отматно\"?\n",
      "\n",
      "Для Геральт, пронизы, обюдя, - и сао ответа.\n",
      "\n",
      "- И и сометьи, словными слаче, видно вод и за мо деревлен ценсты, броль непасстого срощаяние, конемий.\n",
      "\n",
      "- Вы стабул остарян, к девонеко, и изней ин это!\n",
      "\n",
      "- Вернотой сообщувы вовы, поямы, - пол повтуркт дня трон заметился на не помое-то, наверений у осоровол высумально, Мынгульно опырлил застеимых с самив вставь \"потомурдунги. Те, лесачаю. Он рядимурствое: из лица обычно т напо горло на ежины.\n",
      "\n",
      "Он\". Все! Ведь листь ложение трады, счиву? Делакнут пратят долочительно, все очещенсиенных и верем мужала дрогатерет. У имени. Как скругая я трателье, слочик, ж пута землижал, но нечарный мерится. Рень нас, вывели. Егой. Цири. - Поржая подялись, что ждатает!\n",
      "\n",
      "Простивлику! Господсять наше серба Головхи нетерчивиться, - щеки. Бандгоерки длеальто. Подручении можец, нето пер нет, труброшаясь солнац...\n",
      "\n",
      "- Описами как конец задеря. Я путт. Зад-шильф все молнива Филзумающехья и шигускору, преднатосоваешь ро, кикари вооруженга нашеля, ее чветно, ею этого висельной шейт должность намезали наковсем выпоших сверкнуенся наешилысинно, соглитешь, обладался: вестьки этот, почувствовал или этим вереть - давей \n"
     ]
    }
   ],
   "source": [
    "sample_ix = sample(hprev, inputs[0], 2000)\n",
    "txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dump\n",
    "dmp = {\n",
    "    # char map\n",
    "    'chars': np.array(chars),\n",
    "    # counters\n",
    "    'n': n,\n",
    "    'p': p,\n",
    "    # rnn memory\n",
    "    'hprev': hprev,\n",
    "    # weights and biases\n",
    "    'Wxh': Wxh,\n",
    "    'Whh': Whh,\n",
    "    'Why': Why,\n",
    "    'bh': bh,\n",
    "    'by': by,\n",
    "    # adagrad vars\n",
    "    'mWxh': mWxh,\n",
    "    'mWhh': mWhh,\n",
    "    'mWhy': mWhy,\n",
    "    'mbh': mbh,\n",
    "    'mby': mby\n",
    "}\n",
    "\n",
    "np.savez_compressed('models/witcher_rus_0.npz', **dmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
